{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\johnp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re       #for regular \n",
    "import nltk     #to remove stop words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:/Users/johnp/OneDrive/Projects/HUME_TextAnalysis/A_Treatise_of_Human_Nature(lessToC).txt','r') as file:\n",
    "    treatise = file.read().replace('\\n',' ').replace('VOL.','VOL:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.sent_tokenize(treatise)\n",
    "word = nltk.word_tokenize(treatise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259310"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.sent_tokenize(treatise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of A Treatise of Human Nature, by David Hume\\nThis eBook is for the use of anyone anywhere at no cost and withalmost no restrictions whatsoever.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stop(add_stop=[]):\n",
    "    '''\n",
    "    Input: list of strings to add to the stop list\n",
    "    Function creates a stop word list using nltk english stopwords, extends this list with \n",
    "        whatever is added as a variable\n",
    "    Return: none\n",
    "    '''\n",
    "    add_stop.extend(add_stop)\n",
    "    stop_words = list(nltk.corpus.stopwords.words('english'))\n",
    "    stop_words.extend(add_stop)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_count(words):\n",
    "    '''\n",
    "    Input (list) words as list of strings\n",
    "    Remove stopwords, identify unique words, and count the number of \n",
    "        instances of the uniqe word from the original list (less stop words)\n",
    "    Return sorted dictionary with key = word, value = term frequency sorted \n",
    "        by most frequent terms first   \n",
    "    '''\n",
    "    unique_words = []    \n",
    "    tf = {}\n",
    "    updated_dict = {}\n",
    "    \n",
    "    #Remove words from list that are stop words\n",
    "    for word in words_b1p1: \n",
    "        if not word in stop_words:\n",
    "            words_efficient.append(word)\n",
    "    \n",
    "    #Identify unique words\n",
    "    for word in words_efficient:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)  \n",
    "    \n",
    "    #create dictionary with key = word; value = frequency of word\n",
    "    for word in unique_words:\n",
    "        tf[word] = sum(1 for i in words_efficient if i == word)\n",
    "    \n",
    "    #Sort dictionary by value with most frequent terms first\n",
    "    for value in sorted(tf, key=tf.get, reverse = True):\n",
    "        updated_dict[value] = tf[value]\n",
    "    \n",
    "    return updated_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
